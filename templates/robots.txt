# Robots.txt for {{ request.get_host }}
# Generated by Django Sitemap System

# Allow all search engine crawlers
User-agent: *

# Allowed paths
Allow: /
Allow: /products/
Allow: /categories/
Allow: /category/

# Disallow admin and private paths
Disallow: /admin/
Disallow: /mb-admin/
Disallow: /api/
Disallow: /cart/
Disallow: /checkout/
Disallow: /dashboard/
Disallow: /profile/
Disallow: /orders/
Disallow: /wishlist/
Disallow: /login/
Disallow: /register/
Disallow: /logout/
Disallow: /forgot-password/
Disallow: /reset-password/
Disallow: /ckeditor/
Disallow: /backups/
Disallow: /test-integration/
Disallow: /fraud-checker/

# Block specific file types
Disallow: /*.json$
Disallow: /*.xml$

# Crawl-delay (be nice to server)
Crawl-delay: 1

# Sitemap location
Sitemap: {{ request.scheme }}://{{ request.get_host }}/sitemap.xml
